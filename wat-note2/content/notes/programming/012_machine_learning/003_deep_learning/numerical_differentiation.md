---
title: "数値微分法"
date: "2019-10-29T17:33:30+09:00"
description: ""
tags: []
---

前述のフィードフォワードニューラルネットワークにおいて、最適な重み行列w,vの値を求めるために指標となる誤差関数を、教師あり学習の章でも利用した平均誤差エントロピー誤差を利用し、以下の式で定義する。

$$
\tag{1}  E( { \bf w } ,{ \bf v } ) = - \frac{1}{N} \sum_{n=0}^{N-1} \sum_{k=0}^{K-1} t_{nk} \log (y_{nk}) 
$$

tは目標値、yはニューラルネットワークモデルの出力値である。この2つの誤差が大きいほど、誤差関数の値も大きくなる。

式(1)は<b>w</b>,<b>v</b>を入力とするが、この時どのようなw,vを入力すれば誤差関数が最も小さくなるか？を考えた時、教師あり学習のところでも出てきた勾配法を用いて考えてみる。

勾配法の時は偏微分を計算して算出した式を利用していたが、ここでは微分の定義を利用した、近似を利用して偏微分を行なってみよう。

まず誤差関数を<b>w</b>で偏微分することを考えてみる。

関数f(x)をxで微分した値は、ある微小な値εを用いると以下のように表される。

$$
\tag{2}  \frac{　df(x) }{dx} = \frac{f(x + \epsilon )-f(x)}{ \epsilon }
$$

これにより、誤差関数E(<b>w</b>,<b>v</b>)を<b>w</b>で偏微分した値は以下のようになる。

$$
\tag{3}  \frac{　\partial E( { \bf w } ,{ \bf v } ) }{ \partial { \bf w } } = \frac{ E( { \bf w } + \epsilon  ,{ \bf v } )-E( { \bf w }  ,{ \bf v } )}{ \epsilon }
$$

式(3)だが、<b>w</b>には実際にはパラメータが複数ある。例として、<b>w</b>にw<sub>0</sub>,w<sub>1</sub>,w<sub>2</sub> のパラメータがあるとすると、まずw<sub>0</sub>での偏微分は以下のようになる。

$$
\tag{4}  \frac{　\partial E( { \bf w } ,{ \bf v } ) }{ \partial { \bf w } } \Biggr| _{w_{0},w_{1},w_{2} }= \frac{ E( ( w_{0} + \epsilon, w_{1}, w_{2})  ,{ \bf v } )-E( ( w_{0}, w_{1}, w_{2})  ,{ \bf v } )}{ \epsilon }
$$

w<sub>1</sub>,w<sub>2</sub>に対しても同じことを行い、また<b>v</b>のパラメータ全てに対しても行う。

つまりは、<b>w</b>と<b>v</b>のパラメータ全てに対して勾配法を行い、誤差関数を小さくする最適なパタメータの組み合わせを求めていく、というものである。

この手法の難点は、パラメータの数が多いと計算時間が膨大になってしまうという点である。ニューラルネットワークでは重み行列のパラメータの数が多いので、時間も長くなりがちになる。

コードでの実装例を[こちら](https://github.com/WAT36/python/blob/master/machine_learning/deeplearning/numerical_differentialation.ipynb)のNotebookに記載するが、回数にもよるが時間が長くかかりやすいこともわかる。

ここで、別の方法として述べられているものを次に記載する。