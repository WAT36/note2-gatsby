---
title: "混合ガウスモデル"
date: "2019-10-28T20:33:30+09:00"
description: ""
tags: []
---

先程のk-means法では、データを必ずどれか一つのクラスターに割り当てていたが、複数のクラスターに割り当てると言うことはできないだろうか？

ここで、教師あり学習でも利用した、確率を使った表現を利用することを考えてみる。

k-means法では入力データがどのクラスタに属するかを示すものとして行列Rを用いていた。

ここでは、この行列Rを、以下のように行列Γとして設定する。なお、入力データの個数をN個とする。

$$
\tag{1}  {\bf \Gamma}  =  
                \left[
                    \begin{array}{cc}
                    {\bf \gamma}_{0} \\
                    {\bf \gamma}_{1} \\
                    \vdots \\
                    {\bf \gamma}_{N-1} \\
                    \end{array}
                \right]
$$

ここでクラスタ数をKとした時、

$$
\tag{2}  {\bf \gamma}_{i}  =  
                \left[
                    \begin{array}{cc}
                    \gamma_{i0}  & \gamma_{i1} & \cdots & \gamma_{i(K-1)}
                    \end{array}
                \right]
$$

となる。γ<sub>ik</sub>は、i番目の入力データがクラスタkに属する確率を示す。

また、これより

$$
\tag{3}  \sum_{k=0}^{K-1} \gamma_{ik}  =  1
$$

である。

## データがクラスタに属する確率とは？

そもそもの話だが、入力データがクラスタkに属する確率とはどう言う意味なのか？

具体的な例を使って説明してみよう。先程の例では温度とpHを入力としたデータを利用していた。

ここで、例えば温泉の源泉が複数あり、それらの組み合わさったことによって入力データの温度とpHが実現している、と言う説を考えてみよう。

そうなると、源泉の数だけクラスタが存在すると考えることができ、かつ元となる源泉の温度とpHは固定されているので、それらが各クラスタの中心をなしているとも考えられる。

このように、観察はできなかったがデータに影響を与えているだろう変量のことを**潜在変数(latent valiable)**または**隠れ変数(hidden variable)**と呼ぶ。


この潜在変数を数式で定義するにはどうすれば良いだろうか。式(2)と同様にして、潜在変数の個数をK個とした時、以下のようなz<sub>i</sub>を考えてみる。(0≦i＜N)

$$
\tag{4}  {\bf z }_{i}  =  
                \left[
                    \begin{array}{cc}
                    z_{i0}  & z_{i1} & \cdots & z_{i(K-1)}
                    \end{array}
                \right]
$$

式(4)において、z<sub>ij</sub>は、i番目のデータがj番目の潜在変数に属していたら１、属していなければ０をとる変数とする。(0≦j＜K)

これより、γ<sub>ik</sub>（i番目の入力データがクラスタkに属する確率）は、i番目の入力データが潜在変数kに属する確率と言い換えることもでき、以下のような式で表される。

$$
\tag{5}  {\bf \gamma}_{ik}   =  P(z_{ik} = 1 | x_{i})
$$

式(5)は、簡単にいうとz<sub>ik</sub>の値がわからない時に、z<sub>ik</sub>の推定値として利用できる。入力データx<sub>i</sub>がどの潜在変数に属するかわかっていれば、式(5)の値は0または1になるので推定はしなくても良いが、x<sub>i</sub>がどの潜在変数に属するかがわからない場合はこの式(5)の値が0から1の値を取るので、この値が潜在変数に属する確率として利用できる。

この式(5)で表されるγは、「入力データがどの潜在変数(クラスタ)に属する確率」という意味合いから、**負担率 (responsibility)**とも呼ばれている。

確率的にクラスタリングを行うということは、データの背後に潜む潜在変数Zを確率的な変数γ(負担率)として推定することである。

次に、この負担率γを推定する方法についてを述べる。


## 混合ガウスモデル

負担率γを求めるために、混合ガウスモデルという確率モデルを導入する。

混合ガウスモデルは、２次元ガウス関数を複数足し合わせたもので、以下の式で表される。

$$
\tag{6}  p( {\bf x} )  =  \sum_{k=0}^{K-1} \pi_{k} N ({\bf x} | {\bf \mu}_{k} , {\bf \Sigma}_{k} )
$$


N(x|μ<sub>k</sub>,Σ<sub>k</sub>)は平均μ<sub>k</sub>、共分散行列Σ<sub>k</sub>の２次元ガウス関数を表している。

このモデルのパラメータは、各ガウス分布の中心を表す中心ベクトルμ<sub>k</sub>、分布の散らばり具合を示す共分散行列Σ<sub>k</sub>、各ガウス分布の大きさ(係数)を示す**混合係数** π<sub>k</sub>である。混合係数は0から1の間を取る値であり、また全ての混合係数の和は1となる。


$$
\tag{7}  \sum_{k=0}^{K-1} \pi_{k}  =  1
$$

では、どのようにして入力データに対する最適なパラメータμ<sub>k</sub>、Σ<sub>k</sub>、π<sub>k</sub>を求めれば良いか。そのための方法を次に示す。


## EMアルゴリズム

前述のパラメータを入力データにフィットし、負担率γを求める方法として、**EMアルゴリズム**(Expectation-Maximization algorithm)を利用する。この方法はK-means法を応用した方法としても見れる。

K-means法では、各クラスタの中心ベクトルを元にフィッティングを行なったが、EMアルゴリズムでは、中心ベクトルの他に共分散行列もフィッティングに利用する。

大まかな手順を、以下に記載しながら進める。

### 1. パラメータの初期化

まず最初に、パラメータμ、Σ、πを初期化する。

値は何でも良いが、まずはπはどれも均等に、Σは[[1 0],[0 1]]、μはk-means法と同じ要領で決定する。


### 2. γの更新

次に、入力データx<sub>i</sub>と各パラメータμ、Σ、πから、負担率γを計算する。

式(5)から、負担率γを算出する式は以下の式(8)のようになる。

$$
\tag{8}  
\gamma_{ik} = \frac{ \pi_{ik} N  ({\bf x}_{i} | {\bf \mu}_{k} , {\bf \Sigma}_{k} ) }{ \sum_{k=0}^{K-1} \pi_{k} N  ({\bf x}_{i} | {\bf \mu}_{k} , {\bf \Sigma}_{k} )  } 
$$

式(5)は「i番目の入力データが潜在変数kに属する確率」なので、その値は「i番目の入力データが潜在変数0~K-1に属する時のガウス関数の値の和」のうち「i番目の入力データが潜在変数kに属する時のガウス関数の値」の確率になる。それを示したのが式(8)である。

この式により、γ<sub>ik</sub>を一つ一つ求めていく。

ちなみにこの2.の動作を、EMアルゴリズムでは**E Step**(Expectatoin step)と呼ばれる。

### 3. パラメータの更新

次に、算出した負担率γにおいて、各クラスタ(潜在変数)への負担率の和N<sub>k</sub>を求める。

$$
\tag{9}  N_{k} = \sum_{i=0}^{N-1} \gamma_{ik}
$$

このN<sub>k</sub>は、各クラスタに属する入力データの数に相当する。

また、混合係数π<sub>k</sub>を以下の式(10)により更新する。

$$
\tag{10}  \pi_{k} = \frac{ N_{k} }{ N }
$$

Nは入力データの数なので、この式(10)によりクラスタkの混合係数π<sub>k</sub>は全体に対するクラスタ内の数の割合になる。

そして、中心ベクトルμ<sub>k</sub>を以下の式(11)に基づき更新する。

$$
\tag{11}  \mu_{k} = \frac{1}{ N_{k} } \sum_{i=0}^{N-1} \gamma_{ik} x_{i}
$$

式(11)は、クラスタへの負担率を重みづけた上でのデータの平均となり、この値を新しい中心ベクトルとして利用する。

そして次に、共分散行列Σを更新する。Σの更新の際には、式(11)で求めたμ<sub>k</sub>を利用するので注意する。

$$
\tag{12}  \Sigma_{k} = \frac{1}{ N_{k} } \sum_{i=0}^{N-1} \gamma_{ik} (x_{i} - \mu_{k}) (x_{i} - \mu_{k})^{T}
$$

式(12)も式(11)と同じく、計算にはクラスタへの負担率を重みづけた上で行い、出力する。

この3.の動作を、EMアルゴリズムでは**M Step**(Maximization Step)と呼ぶ。

### 4.  2.へ戻る(E Stepを行う)

M stepが終わったらE stepを行い、その後M stepを行い、またE stepを行い、、というように、E stepとM stepを繰り返していき、パラメータが収束するまで行う。


以上が、混合ガウスモデル、及びEMアルゴリズムによる確率的クラスタリングの算出である。


# 尤度

EMアルゴリズムでは混合ガウスモデルが入力データに合うようにパラメータを最適化させていたが、パラメータごとではない、EMアルゴリズムによって最適化される値というのは何かないだろうか。

実は、それは尤度と呼ばれるもので、入力データが混合ガウスモデルから生成される確率である（教師あり学習　の章で既出）

尤度の式は、下式(13)で与えられる。

$$
\tag{13}  p( {\bf X} | {\bf \pi} , {\bf \mu} , {\bf \Sigma} ) =  \prod_{n=0}^{N-1} \sum_{k=0}^{K-1} \pi_{k} N (x_{n} | \mu_{k} , \Sigma_{k})
$$

対数尤度をとると

$$
\tag{14}  \log p( {\bf X} | {\bf \pi} , {\bf \mu} , {\bf \Sigma} ) =  \sum_{n=0}^{N-1} \{ \log \sum_{k=0}^{K-1} \pi_{k} N (x_{n} | \mu_{k} , \Sigma_{k}) \}
$$

値を最大化(最適化のため)させるために、式(14)に-1をかけて負の対数尤度にし、それを誤差関数E(π,μ,Σ)として定義すると

$$
\tag{15}  
\begin{aligned}
E ( \pi , \mu , \Sigma) 
    &= - \log p( {\bf X} | {\bf \pi} , {\bf \mu} ) \\
    &= - \sum_{n=0}^{N-1} \{ \log \sum_{k=0}^{K-1} \pi_{k} N (x_{n} | \mu_{k} , \Sigma_{k}) \}
\end{aligned}
$$

となる。最適化すればするほど、この誤差関数の値が小さくなっていく。

この誤差関数を指標とすることで、混合ガウスモデル、EMアルゴリズムが正しく動作されているかを確認できる。