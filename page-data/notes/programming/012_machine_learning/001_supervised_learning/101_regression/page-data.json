{"componentChunkName":"component---src-templates-note-dir-js","path":"/notes/programming/012_machine_learning/001_supervised_learning/101_regression","result":{"data":{"site":{"siteMetadata":{"title":"WAT Note(II)"}},"allDirectory":{"nodes":[]},"allMarkdownRemark":{"nodes":[{"excerpt":"回帰問題の手法の一つである直線モデルについて。 教師あり学習 で与えられた入力と結果から関係式を算出すると言うことを述べたが、\n必ずしも100%正確な式を算出すると言うことでは無いことを念頭においてほしい。（と言うよりどのような入力に対して10…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/linear_model/"},"frontmatter":{"date":"October 26, 2019","title":"直線モデル(線形回帰)","description":""}},{"excerpt":"前述の直線モデルにて、データに応じた最適なw0,w1を決めれば最適な関係式が得られると書いたが、\nどのようにして最適なw0,w…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/mse/"},"frontmatter":{"date":"October 26, 2019","title":"平均二乗誤差","description":""}},{"excerpt":"前述の平均二乗誤差で最も誤差が小さくなるw0,w1を正確に求めるにはどうすればよいのか？ その一例として、ここでは勾配法という方法についてを述べる。 勾配法とは曲面(…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/steepest_descent_method/"},"frontmatter":{"date":"October 26, 2019","title":"勾配法(最急降下法)","description":""}},{"excerpt":"先程の直線モデルの例では勾配法を用いてw0、w1の値を求めたが、実は勾配法を用いなくても数式を計算していけば最適なw0、w…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/analytical_solution/"},"frontmatter":{"date":"October 26, 2019","title":"解析解","description":""}},{"excerpt":"先程の勾配法の例では入力データは1次元であったが、2次元であった場合はどうだろうか。 例として、以下のようなデータを用意する。 年齢 身長 体重 6 117.9 28.0 16 164.3 58.2 19 171.6 60.1 18 172.7 65.2 9 132.8 35.…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/2dmodel/"},"frontmatter":{"date":"October 26, 2019","title":"2次元入力の面モデル","description":""}},{"excerpt":"先程までの章では1次元(直線モデル)、2次元(面モデル)の入力データを扱ってきたが、そこから更に次元を広げたN次元の入力データの場合はどうなるだろうか。\nここではそれについてを述べる。 N次元での入力データから予測値yを算出する式は以下の式で表される。 N次元の時も1次元…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/N-dimension_linear_model/"},"frontmatter":{"date":"October 26, 2019","title":"N次元線形回帰モデル","description":""}},{"excerpt":"…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/linear_basis_function/"},"frontmatter":{"date":"October 26, 2019","title":"線形基底関数モデル","description":""}},{"excerpt":"先述の線形基底関数モデルにおいて、M…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/overfitting/"},"frontmatter":{"date":"October 26, 2019","title":"オーバーフィッティング(過学習)","description":""}},{"excerpt":"先述のオーバーフィッティング(過学習)の章で、Mを大きくすればするほど既存の入力データに対する精度が高くなり、未知の入力に対する予測精度が悪くなるという問題があった。最適なM…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/holdout_validation/"},"frontmatter":{"date":"October 26, 2019","title":"ホールドアウト検証","description":""}},{"excerpt":"…","fields":{"slug":"/programming/012_machine_learning/001_supervised_learning/101_regression/cross_validation/"},"frontmatter":{"date":"October 26, 2019","title":"交差検証","description":""}}]}},"pageContext":{"absolutePath":"/Users/watarutsukagoshi/Desktop/WTFiles/gatsby-blog/wat-note2/content/notes/programming/012_machine_learning/001_supervised_learning/101_regression","markdownRegexPath":"//Users/watarutsukagoshi/Desktop/WTFiles/gatsby-blog/wat-note2/content/notes/programming/012_machine_learning/001_supervised_learning/101_regression/[^/]*.md/"}},"staticQueryHashes":["2841359383","3257411868"]}